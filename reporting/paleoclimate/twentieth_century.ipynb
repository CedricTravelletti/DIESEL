{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d63573-85f8-425e-b325-feecb96de037",
   "metadata": {},
   "source": [
    "# Assimilate GLSD data with DIESEL for 20th century.\n",
    "\n",
    "This notebook runs assimilation of GLSD data using the DIESEL version of the Ensemble Kalman filter. It compares sequential and all-at-once assimilation on the whole 20th century."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5b5043-2a78-4111-859b-e59950f8947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/ct19x463/.conda/envs/climate/lib/python3.8/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import dask\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "from climate.utils import load_dataset, match_vectors_indices\n",
    "\n",
    "\n",
    "from dask.distributed import Client, wait, progress                             \n",
    "import diesel as ds                                                             \n",
    "from diesel.scoring import compute_RE_score, compute_CRPS, compute_energy_score \n",
    "from diesel.estimation import localize_covariance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6eb785c-8692-41cf-94a4-705d89e4e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"/storage/homefs/ct19x463/Dev/Climate/Data/\"\n",
    "results_folder = \"/storage/homefs/ct19x463/Dev/DIESEL/reporting/paleoclimate/results/twentieth_century/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad792b7b-ea36-4515-a2ff-e427f3de441b",
   "metadata": {},
   "source": [
    "## Build Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4abf061-64dd-495c-ba16-78a39ae1d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = ds.cluster.UbelixCluster(n_nodes=12, mem_per_node=64, cores_per_node=3,\n",
    "            partition=\"gpu\", qos=\"job_gpu\")                                     \n",
    "cluster.scale(18)                                                           \n",
    "client = Client(cluster)                                                    \n",
    "                                                                                \n",
    "# Add to builtins so we have one global client.\n",
    "# Note that this is necessary before importing the EnsembleKalmanFilter module, so that the module is aware of the cluster.\n",
    "__builtins__.CLIENT = client                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f22768-5a73-4a98-84c7-b2e108260860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diesel.kalman_filtering import EnsembleKalmanFilter \n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10014e1-c7bd-4096-a316-847918c128a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295fc0ebca6a4a00944e4b55c7be7d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-outpuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287f852a-e1b8-4ea2-8b7a-42f95c9836ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/ct19x463/.conda/envs/climate/lib/python3.8/site-packages/xarray/coding/times.py:351: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  sample = dates.ravel()[0]\n",
      "/storage/homefs/ct19x463/Dev/Climate/climate/utils.py:144: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, '360_day', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  dataset_mean['time'] = dataset_mean.indexes['time'].to_datetimeindex()\n",
      "/storage/homefs/ct19x463/Dev/Climate/climate/utils.py:145: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, '360_day', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  dataset_members['time'] = dataset_members.indexes['time'].to_datetimeindex()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading done.\n"
     ]
    }
   ],
   "source": [
    "TOT_ENSEMBLES_NUMBER = 30\n",
    "(dataset_mean, dataset_members,\n",
    "    dataset_instrumental, dataset_reference,\n",
    "    dataset_members_zarr)= load_dataset(\n",
    "    base_folder, TOT_ENSEMBLES_NUMBER, ignore_members=True)\n",
    "print(\"Loading done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86679073-abd2-4df3-9564-98c2de441380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal distance to matched point: 120.54565778878536 km.\n"
     ]
    }
   ],
   "source": [
    "from climate.kalman_filter import EnsembleKalmanFilterScatter\n",
    "helper_filter = EnsembleKalmanFilterScatter(dataset_mean, dataset_members_zarr, dataset_instrumental, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da33fd3-0c35-462b-8e7f-7eef582dc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = EnsembleKalmanFilter()                                      \n",
    "data_std = 0.1\n",
    "year = 1901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38407738-b809-42c0-9a27-42575807b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All at once.\n",
    "for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "    # Prepare vectors.\n",
    "    assimilation_date = '{}-{}-16'.format(year, month)\n",
    "    mean_ds = helper_filter.dataset_mean.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "    ensemble_ds = helper_filter.dataset_members.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "    \n",
    "    mean_ds, ensemble_ds = client.persist(mean_ds), client.persist(ensemble_ds)\n",
    "    \n",
    "    # Load data.\n",
    "    data_df = pd.read_csv(os.path.join(base_folder, \"Instrumental/GLSD/yearly_csv/temperature_{}.csv\".format(year)), index_col=0)\n",
    "    data_ds = xr.Dataset.from_dataframe(data_df)\n",
    "\n",
    "    # Rename the date variable and make latitude/longitude into coordinates.\n",
    "    data_ds = data_ds.rename({'date': 'time'})\n",
    "    data_ds = data_ds.set_coords(['time', 'latitude', 'longitude'])\n",
    "    data_ds = data_ds['temperature']\n",
    "    \n",
    "    # Prepare forward.\n",
    "    date= '{}-{}-01'.format(year, month)\n",
    "    data_month_ds = data_ds.where(data_ds.time==date, drop=True)\n",
    "\n",
    "    # Need to clean data since dataset contains erroneous measurements, i.e. \n",
    "    # either extreme values (10^30) or values that are exactly zero for a given station across time.\n",
    "    data_month_ds = data_month_ds.where((data_month_ds > -100.0) & (data_month_ds < 100.0) & (da.abs(data_month_ds) > 0.0001), drop=True)\n",
    "    data_vector = client.persist(da.from_array(data_month_ds.data))\n",
    "\n",
    "    \n",
    "    # Get the model cell index corresponding to each observations.\n",
    "    matched_inds = match_vectors_indices(mean_ds, data_month_ds)\n",
    "\n",
    "    # WARNING: Never try to execute bare loops in DASK, it will exceed the maximal graph depth.\n",
    "    G = np.zeros((data_month_ds.shape[0], mean_ds.shape[0]))\n",
    "    for obs_nr, model_cell_ind in enumerate(matched_inds):\n",
    "        G[obs_nr, model_cell_ind] = 1.0\n",
    "\n",
    "    G = da.from_array(G)\n",
    "    G = client.persist(G)\n",
    "    \n",
    "    # Estimate covariance.\n",
    "    raw_estimated_cov_lazy = ds.estimation.empirical_covariance(ensemble_ds.chunk((1, 1800)))  \n",
    "                                                                                \n",
    "    # Persist the covariance on the cluster.                                \n",
    "    raw_estimated_cov = client.persist(raw_estimated_cov_lazy) \n",
    "    progress(raw_estimated_cov)\n",
    "    \n",
    "    # Construct (lazy) covariance matrix.                                       \n",
    "    lambda0 = 1500 # Localization in kilometers.\n",
    "    lengthscales = da.from_array([lambda0])   \n",
    "    kernel = ds.covariance.squared_exponential(lengthscales)\n",
    "    \n",
    "    # Build localization matrix.\n",
    "    grid_pts = da.vstack([mean_ds.latitude, mean_ds.longitude]).T\n",
    "    grid_pts = client.persist(grid_pts.rechunk((1800, 2)))\n",
    "    localization_matrix = kernel.covariance_matrix(grid_pts, grid_pts, metric='haversine') \n",
    "    localization_matrix = client.persist(localization_matrix)\n",
    "    progress(localization_matrix)\n",
    "    \n",
    "    # Localize covariance.\n",
    "    loc_estimated_cov = localize_covariance(raw_estimated_cov, localization_matrix)\n",
    "    loc_estimated_cov = client.persist(loc_estimated_cov)\n",
    "    progress(loc_estimated_cov)\n",
    "    \n",
    "    # Assimilate all data.\n",
    "    mean_updated_aao, ensemble_updated_aao = my_filter.update_ensemble(\n",
    "        mean_ds.data, ensemble_ds.data, G,\n",
    "        data_vector, data_std, loc_estimated_cov)\n",
    "\n",
    "    # Trigger computations and block. Otherwise will clutter the scheduler. \n",
    "    mean_updated_aao = client.persist(mean_updated_aao)                \n",
    "    ensemble_updated_aao = client.persist(ensemble_updated_aao)\n",
    "    progress(ensemble_updated_aao) # Block till end of computations.        \n",
    "    \n",
    "    # Save data.\n",
    "    np.save(os.path.join(results_folder, \"mean_updated_aao_{}.npy\".format(date)),\n",
    "        mean_updated_aao.compute())\n",
    "    np.save(os.path.join(results_folder, \"ensemble_updated_aao_{}.npy\".format(date)),\n",
    "        ensemble_updated_aao.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd91b52-1b06-41cf-b7fc-2ddde6994c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct localization matrix.                                      \n",
    "lambda0 = 1500 # Localization in kilometers.\n",
    "lengthscales = da.from_array([lambda0])   \n",
    "kernel = ds.covariance.squared_exponential(lengthscales)\n",
    "    \n",
    "# Build localization matrix.\n",
    "mean_dummy = helper_filter.dataset_mean.get_window_vector('1961-01-16', '1961-01-16', variable='temperature') # Dummy, just to get the grid.\n",
    "\n",
    "grid_pts = da.vstack([mean_dummy.latitude, mean_dummy.longitude]).T\n",
    "grid_pts = client.persist(grid_pts.rechunk((1800, 2)))\n",
    "localization_matrix = kernel.covariance_matrix(grid_pts, grid_pts, metric='haversine') \n",
    "localization_matrix = client.persist(localization_matrix)\n",
    "progress(localization_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64c0039-ceff-4a9a-960c-10faa2b7db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/ct19x463/.conda/envs/climate/lib/python3.8/site-packages/dask/array/blockwise.py:288: UserWarning: The da.atop function has moved to da.blockwise\n",
      "  warnings.warn(\"The da.atop function has moved to da.blockwise\")\n",
      "/storage/homefs/ct19x463/.conda/envs/climate/lib/python3.8/site-packages/dask/array/blockwise.py:289: PerformanceWarning: Increasing number of chunks by factor of 11\n",
      "  return blockwise(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "Maximal distance to matched point: 113.08002097917435 km.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Now sequential.\n",
    "for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "    # Prepare vectors.\n",
    "    assimilation_date = '{}-{}-16'.format(year, month)\n",
    "    mean_ds = helper_filter.dataset_mean.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "    ensemble_ds = helper_filter.dataset_members.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "    \n",
    "    mean_ds, ensemble_ds = client.persist(mean_ds), client.persist(ensemble_ds)\n",
    "    \n",
    "    # Load data.\n",
    "    data_df = pd.read_csv(os.path.join(base_folder, \"Instrumental/GLSD/yearly_csv/temperature_{}.csv\".format(year)), index_col=0)\n",
    "    data_ds = xr.Dataset.from_dataframe(data_df)\n",
    "\n",
    "    # Rename the date variable and make latitude/longitude into coordinates.\n",
    "    data_ds = data_ds.rename({'date': 'time'})\n",
    "    data_ds = data_ds.set_coords(['time', 'latitude', 'longitude'])\n",
    "    data_ds = data_ds['temperature']\n",
    "    \n",
    "    # Prepare forward.\n",
    "    date= '{}-{}-01'.format(year, month)\n",
    "    data_month_ds = data_ds.where(data_ds.time==date, drop=True)\n",
    "\n",
    "    # Need to clean data since dataset contains erroneous measurements, i.e. \n",
    "    # either extreme values (10^30) or values that are exactly zero for a given station across time.\n",
    "    data_month_ds = data_month_ds.where((data_month_ds > -100.0) & (data_month_ds < 100.0) & (da.abs(data_month_ds) > 0.0001), drop=True)\n",
    "    data_vector = client.persist(da.from_array(data_month_ds.data))\n",
    "\n",
    "    \n",
    "    # Get the model cell index corresponding to each observations.\n",
    "    matched_inds = match_vectors_indices(mean_ds, data_month_ds)\n",
    "\n",
    "    # WARNING: Never try to execute bare loops in DASK, it will exceed the maximal graph depth.\n",
    "    G = np.zeros((data_month_ds.shape[0], mean_ds.shape[0]))\n",
    "    for obs_nr, model_cell_ind in enumerate(matched_inds):\n",
    "        G[obs_nr, model_cell_ind] = 1.0\n",
    "\n",
    "    G = da.from_array(G)\n",
    "    G = client.persist(G)\n",
    "    \n",
    "    # Assimilate all data.\n",
    "    mean_updated_seq, ensemble_updated_seq = my_filter.update_ensemble_sequential_nondask(\n",
    "        mean_ds.data, ensemble_ds.data, G,\n",
    "        data_vector, data_std, localization_matrix)\n",
    "    \n",
    "    # Save data.\n",
    "    np.save(os.path.join(results_folder, \"mean_updated_seq_{}.npy\".format(date)),\n",
    "        mean_updated_seq)\n",
    "    np.save(os.path.join(results_folder, \"ensemble_updated_seq_{}.npy\".format(date)),\n",
    "        ensemble_updated_seq)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dff9cdf-9a40-41d8-b39d-efee6ed141b8",
   "metadata": {},
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e9690de-16bf-46b6-841a-cf19ff5c020e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1816"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecc54b-439a-461c-9a7b-13cb2c419bc8",
   "metadata": {},
   "source": [
    "# Run Assimilation: All-at-once (aao) vs sequential (seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296b30c-54b4-467b-b818-a9c1c9b2f8a7",
   "metadata": {},
   "source": [
    "## Compare the different updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85c269-d963-417e-9908-fbc8c96017bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plotting functions.\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "import cartopy.crs as ccrs\n",
    "from shapely import geometry\n",
    "\n",
    "def plot(unstacked_data, ax, outfile=None, vmin=None, vmax=None):\n",
    "    # ax = plt.axes(projection=ccrs.Mollweide())\n",
    "    # ax.set_global()\n",
    "    unstacked_data.plot.contourf(levels=30, ax=ax, transform=ccrs.PlateCarree(),\n",
    "                                vmin=vmin, vmax=vmax, cmap='RdBu_r',\n",
    "                                 add_colorbar=False, add_labels=False,\n",
    "                               #cbar_kwargs={'ticks': [-30, -20, -10, 0, 10, 20, 30],\n",
    "                                           # 'label': 'temperature'}\n",
    "                                 extend='both',\n",
    "                                )\n",
    "    # Center on Europe\n",
    "    ax.set_extent([-25, 30, 30, 75], crs=ccrs.PlateCarree())\n",
    "    ax.coastlines() \n",
    "    ax.set_title('')\n",
    "    ax.set_ylabel('')\n",
    "    if outfile is not None: plt.savefig(outfile, bbox_inches='tight', dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2aeab2-4a85-4ad2-a772-e384df5a2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 1/2.54  # centimeters in inches\n",
    "fig, axs = plt.subplots(6, 3, figsize=(60*cm, 50*cm),\n",
    "                       subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "for i, month in enumerate(['01', '02', '03', '04', '05', '06']):\n",
    "    mean_updated_aao = np.load(os.path.join(results_folder, 'mean_updated_aao_1816-{}-01.npy'.format(month)))\n",
    "    mean_updated_seq = np.load(os.path.join(results_folder, 'mean_updated_seq_1816-{}-01.npy'.format(month)))\n",
    "    \n",
    "    unstacked_updated_mean_aao = helper_filter.dataset_mean.unstack_window_vector(mean_updated_aao, time='1816-{}-16'.format(month), variable_name='temperature')\n",
    "    unstacked_updated_mean_seq = helper_filter.dataset_mean.unstack_window_vector(mean_updated_seq, time='1816-{}-16'.format(month), variable_name='temperature')\n",
    "    ref = dataset_reference.temperature.sel(time='1816-{}-16'.format(month))\n",
    "    \n",
    "    plot(unstacked_updated_mean_aao, axs[i, 0], vmin=-20, vmax=30)\n",
    "    plot(unstacked_updated_mean_seq, axs[i, 1], vmin=-20, vmax=30)    \n",
    "    plot(ref, axs[i, 2], vmin=-20, vmax=30)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b83a7c-1fb0-428a-8f0d-259f4fec3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_updated_mean_aao = helper_filter.dataset_mean.unstack_window_vector(mean_updated_aao.compute(), time='1816-01-16', variable_name='temperature')\n",
    "plot(unstacked_updated_mean_aao, vmin=-40, vmax=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398995a3-5eca-485c-bcb1-f09fb066b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_updated_ensemble_0_aao = helper_filter.dataset_mean.unstack_window_vector(ensemble_updated_aao[0, :].compute(), time='1961-01-16', variable_name='temperature')\n",
    "plot(unstacked_updated_ensemble_0_aao, vmin=-40, vmax=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed872d-6cca-4ef9-83d7-ff5ae8045d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_updated_mean_seq = helper_filter.dataset_mean.unstack_window_vector(mean_updated_seq, time='1961-01-16', variable_name='temperature')\n",
    "plot(unstacked_updated_mean_seq, vmin=-40, vmax=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9b724-3242-4b01-9cc1-a66dbc20c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference.\n",
    "plot(unstacked_updated_mean_aao - unstacked_updated_mean_seq, vmin=-7, vmax=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f05a39-1b0f-41c6-98a4-5394708c4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data (before updating.\n",
    "unstacked_mean = helper_filter.dataset_mean.unstack_window_vector(mean_ds.values.reshape(-1), time='1961-01-16', variable_name='temperature')\n",
    "plot(unstacked_mean.temperature, vmin=-40, vmax=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e7e6a-7c51-4367-8f1f-af2512da887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot station data.\n",
    "df = data_month_ds.to_dataframe()\n",
    "# Could reset coordinates if you really wanted\n",
    "# df = df.reset_index()\n",
    "cm = 1/2.54  # centimeters in inches\n",
    "fig = plt.figure(figsize=(40*cm, 25*cm))\n",
    "ax = plt.axes(projection=ccrs.Mollweide())\n",
    "ax.set_global()\n",
    "    \n",
    "ax.coastlines()  \n",
    "\n",
    "df.plot.scatter('longitude', 'latitude', c=data_month_ds.name, cmap='jet', ax=ax, transform=ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ed9b2-3b83-4c61-a331-2e1cd46f6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error wrt reference.\n",
    "plot(unstacked_updated_mean_aao - dataset_reference.temperature.sel(time='1961-01-16'), vmin=-7, vmax=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d003d-aa35-4f0f-9f07-56c734ba4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(unstacked_updated_mean_seq - dataset_reference.temperature.sel(time='1961-01-16'), vmin=-7, vmax=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2ad55-f006-4d05-96cd-8fff375ceeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original error.\n",
    "plot(unstacked_mean.temperature - dataset_reference.temperature.sel(time='1961-01-16'), vmin=-7, vmax=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c96e1-9007-47ce-a7af-18556b4f6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_filter.dataset_members.dataset_members.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd282313-2a5e-41b4-bfa3-f3c5dc318937",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_reference.temperature.sel(time='1816-12-16') - dataset_reference.temperature.sel(time='1900-06-16')).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfd694-65d4-4558-8e8b-db2d6bd15a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f31a7-8666-4206-a7ba-fb185c4c7b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
