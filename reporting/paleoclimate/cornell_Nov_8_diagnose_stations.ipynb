{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aeb4e26-e963-41c5-a273-754ab850f5b9",
   "metadata": {},
   "source": [
    "# Diagnose stations.\n",
    "\n",
    "The twentieth_century_station script, which is supposed to be our current state of the art, fails miserably. \n",
    "The prior is within 0.5 degrees of the data (RMSE), while after assimilation we are way off.\n",
    "\n",
    "This notebook aims at diagnosing what goes wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97aef55-38e0-4dab-8cbc-ce1a73367af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly copied from the script, to set things up.\n",
    "import os\n",
    "import numpy as np\n",
    "import dask\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "from climate.utils import load_dataset, match_vectors_indices\n",
    "from climate.data_wrapper import StationDataset\n",
    "\n",
    "\n",
    "from dask.distributed import Client, wait, progress                             \n",
    "import diesel as ds                                                             \n",
    "from diesel.scoring import compute_RE_score, compute_CRPS, compute_energy_score, compute_RMSE\n",
    "from diesel.estimation import localize_covariance \n",
    "from diesel.utils import build_forward_mean_per_cell\n",
    "\n",
    "\n",
    "base_folder = \"/storage/homefs/ct19x463/Dev/Climate/Data/\"\n",
    "results_folder = \"/storage/homefs/ct19x463/Dev/DIESEL/reporting/paleoclimate/results/twentieth_century/stations/\"\n",
    "\n",
    "# Build Cluster\n",
    "cluster = ds.cluster.UbelixCluster(n_nodes=12, mem_per_node=64, cores_per_node=3,\n",
    "            partition=\"gpu\", qos=\"job_gpu\")                                     \n",
    "cluster.scale(18)                                                           \n",
    "client = Client(cluster)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e6861-67e5-4915-962d-f6ce49c004f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fee6e7-aa21-4eee-9d0c-1a4dbe0d6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to builtins so we have one global client.\n",
    "# Note that this is necessary before importing the EnsembleKalmanFilter module, so that the module is aware of the cluster.\n",
    "__builtins__.CLIENT = client                                                \n",
    "\n",
    "\n",
    "from diesel.kalman_filtering import EnsembleKalmanFilter \n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "TOT_ENSEMBLES_NUMBER = 30\n",
    "(dataset_mean, dataset_members,\n",
    "    dataset_instrumental, dataset_reference,\n",
    "    dataset_members_zarr)= load_dataset(\n",
    "    base_folder, TOT_ENSEMBLES_NUMBER, ignore_members=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97650df1-63b6-4717-a6cc-9134f0515235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stationDataset = StationDataset(base_folder)\n",
    "print(\"Loading done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c95897-7c40-45b4-812a-9dc138f2efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climate.kalman_filter import EnsembleKalmanFilterScatter\n",
    "helper_filter = EnsembleKalmanFilterScatter(dataset_mean, dataset_members_zarr, dataset_instrumental, client)\n",
    "\n",
    "my_filter = EnsembleKalmanFilter()                                      \n",
    "data_std = 0.1\n",
    "\n",
    "# Construct localization matrix.                                      \n",
    "lambda0 = 1500 # Localization in kilometers.\n",
    "lengthscales = da.from_array([lambda0])   \n",
    "kernel = ds.covariance.squared_exponential(lengthscales)\n",
    "    \n",
    "# Build localization matrix.\n",
    "mean_dummy = helper_filter.dataset_mean.get_window_vector('1961-01-16', '1961-01-16', variable='temperature') # Dummy, just to get the grid.\n",
    "\n",
    "grid_pts = da.vstack([mean_dummy.latitude, mean_dummy.longitude]).T\n",
    "grid_pts = client.persist(grid_pts.rechunk((1800, 2)))\n",
    "localization_matrix = kernel.covariance_matrix(grid_pts, grid_pts, metric='haversine') \n",
    "localization_matrix = client.persist(localization_matrix)\n",
    "progress(localization_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b5500-78c2-435d-b23d-fdf4c789c808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea248242-c12c-4540-9e26-cf70b871d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "year, month = '1982', '07'\n",
    "assimilation_date = '{}-{}-16'.format(year, month)\n",
    "\n",
    "mean_ds = helper_filter.dataset_mean.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "ensemble_ds = helper_filter.dataset_members.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "    \n",
    "mean_ds, ensemble_ds = client.persist(mean_ds), client.persist(ensemble_ds)\n",
    "\n",
    "# Get anomaly.\n",
    "anomaly = helper_filter.dataset_mean.get_window_vector(assimilation_date, assimilation_date, variable='anomaly')\n",
    "climatology = mean_ds - anomaly\n",
    "\n",
    "ensemble_anomaly = ensemble_ds.data - climatology.data.reshape(-1)[None, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af0fce-12ef-4131-880c-588ae5189969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "data = stationDataset.get_station_data(year, month, \"16\")\n",
    "data_df = pd.DataFrame(data, columns = ['temperature', 'climatology','latitude','longitude'])\n",
    "data_ds = xr.Dataset.from_dataframe(data_df)\n",
    "\n",
    "# Rename the date variable and make latitude/longitude into coordinates.\n",
    "data_ds = data_ds.set_coords(['latitude', 'longitude'])\n",
    "    \n",
    "# data_month_ds = data_month_ds.where((data_month_ds > -100.0) & (data_month_ds < 100.0) & (da.abs(data_month_ds) > 0.0001), drop=True)\n",
    "data_ds['anomaly'] = (data_ds['temperature'] - data_ds['climatology'])\n",
    "        \n",
    "# Build cell-averaged forward.\n",
    "G_mean, d_mean, d_lons, d_lats = build_forward_mean_per_cell(mean_ds, data_ds['anomaly'])\n",
    "G_mean = client.persist(da.from_array(G_mean))\n",
    "d_mean = client.persist(da.from_array(d_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b2193-c237-4628-a9f2-289860c4de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_data = G_mean @ anomaly.values\n",
    "print((pred_data - d_mean).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0320bf-be7f-4b10-b880-2534934d7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HadCRUT reference\n",
    "ref_ds = xr.open_dataset(os.path.join(base_folder, \"Reference/HadCRUT.5.0.1.0.analysis.anomalies.ensemble_mean.nc\"))\n",
    "if month == '02':\n",
    "    ref_date = '{}-{}-15'.format(year, month)\n",
    "else: ref_date = assimilation_date\n",
    "ref = ref_ds['tas_mean'].sel(time=ref_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a0578-16bb-4cc0-836e-9481e4835414",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Regrid to common extent.\n",
    "unstacked_prior = helper_filter.dataset_mean.unstack_window_vector(anomaly.values, time=assimilation_date, variable_name='temperature')\n",
    "regridded_prior = unstacked_prior.interp(latitude=ref.latitude).interp(longitude=ref.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8875a-5883-436b-b491-556be2a5d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now restack.\n",
    "stacked_ref = ref.stack(stacked_dim=('latitude', 'longitude')).isel(time=0).compute()\n",
    "stacked_prior = regridded_prior.stack(stacked_dim=('latitude', 'longitude')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd157864-e59f-4228-a073-4d40af7f7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((stacked_ref - stacked_prior).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980ea03-43c1-4b56-886c-7de0af6820f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the forward on the restacked grid.\n",
    "# Build cell-averaged forward.\n",
    "G_mean, d_mean = build_forward_mean_per_cell(stacked_prior, data_ds['anomaly'])\n",
    "G_mean = client.persist(da.from_array(G_mean))\n",
    "d_mean = client.persist(da.from_array(d_mean))\n",
    "print((G_mean @ stacked_ref.values).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999a24d-58b3-4a11-847a-1a91b55da576",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_inds = stacked_ref.isnull().compute()\n",
    "vals = stacked_ref.values\n",
    "vals[nan_inds] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0650c0-ce11-4646-8cb3-9e172d1ade71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conclusion: the regridding to a coarse resolution (that of the reference) does not work well with the data (too much averaging).\n",
    "print(((G_mean @ vals).compute() - d_mean).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e1a87-7527-4071-b59a-4c93cd8f4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try other solution: regrid to finer grid instead.\n",
    "# Regrid to common extent.\n",
    "unstacked_anomaly = helper_filter.dataset_mean.unstack_window_vector(anomaly.values, time=assimilation_date, variable_name='anomaly')\n",
    "\n",
    "regridded_ref = ref.isel(time=0).interp(latitude=unstacked_anomaly.latitude).interp(longitude=unstacked_anomaly.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0208c63-964f-4a03-a8bd-46b0482097bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if now predict correctly.\n",
    "stacked_ref = regridded_ref.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "\n",
    "nan_inds = stacked_ref.isnull().compute()\n",
    "vals = stacked_ref.values\n",
    "vals[nan_inds] = 0.0\n",
    "diffs = ((G_mean @ vals).compute() - d_mean).compute()\n",
    "import seaborn as sns\n",
    "sns.histplot(diffs, kde=True)\n",
    "\n",
    "prior_diffs = ((G_mean @ anomaly.values).compute() - d_mean).compute()\n",
    "sns.histplot(prior_diffs, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedabd23-7bfc-4f15-aeea-77340ad124e9",
   "metadata": {},
   "source": [
    "# Try to run one round of assimilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ef8c4-25c5-4b37-8563-b82fc92c5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_prior, ES_aao_loc, ES_seq_loc = [], [], []        \n",
    "RE_aao_loc, RE_seq_loc = [], []                       \n",
    "RMSE_prior, RMSE_aao_loc, RMSE_seq_loc = [], [], []\n",
    "\n",
    "dates, months, years = [], [], []\n",
    "\n",
    "\n",
    "# Loop over years.\n",
    "for year in range(1990, 1991):\n",
    "## Loop over months.\n",
    "    for month in ['01']:\n",
    "        # Prepare vectors.\n",
    "        assimilation_date = '{}-{}-16'.format(year, month)\n",
    "        mean_ds = helper_filter.dataset_mean.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "        ensemble_ds = helper_filter.dataset_members.get_window_vector(assimilation_date, assimilation_date, variable='temperature')\n",
    "    \n",
    "        mean_ds, ensemble_ds = client.persist(mean_ds), client.persist(ensemble_ds)\n",
    "\n",
    "        # Get anomaly.\n",
    "        anomaly = helper_filter.dataset_mean.get_window_vector(assimilation_date, assimilation_date, variable='anomaly')\n",
    "        climatology = mean_ds - anomaly\n",
    "\n",
    "        ensemble_anomaly = ensemble_ds.data - climatology.data.reshape(-1)[None, :]\n",
    "        \n",
    "        # Load data.\n",
    "        data = stationDataset.get_station_data(year, month, \"16\")\n",
    "        data_df = pd.DataFrame(data, columns = ['temperature', 'climatology','latitude','longitude'])\n",
    "        data_ds = xr.Dataset.from_dataframe(data_df)\n",
    "\n",
    "        # Rename the date variable and make latitude/longitude into coordinates.\n",
    "        data_ds = data_ds.set_coords(['latitude', 'longitude'])\n",
    "    \n",
    "       # data_month_ds = data_month_ds.where((data_month_ds > -100.0) & (data_month_ds < 100.0) & (da.abs(data_month_ds) > 0.0001), drop=True)\n",
    "        data_ds['anomaly'] = (data_ds['temperature'] - data_ds['climatology'])\n",
    "        \n",
    "        # Build cell-averaged forward.\n",
    "        G_mean, d_mean, d_lons, d_lats = build_forward_mean_per_cell(mean_ds, data_ds['anomaly'])\n",
    "        G_mean = client.persist(da.from_array(G_mean))\n",
    "        d_mean = client.persist(da.from_array(d_mean))\n",
    "    \n",
    "        # Estimate covariance.\n",
    "        raw_estimated_cov_lazy = ds.estimation.empirical_covariance(ensemble_ds.chunk((1, 1800)))                                                                               \n",
    "        # Persist the covariance on the cluster.                                \n",
    "        raw_estimated_cov = client.persist(raw_estimated_cov_lazy) \n",
    "        progress(raw_estimated_cov)\n",
    "    \n",
    "        # Localize covariance.\n",
    "        loc_estimated_cov = localize_covariance(raw_estimated_cov, localization_matrix)\n",
    "        loc_estimated_cov = client.persist(loc_estimated_cov)\n",
    "        progress(loc_estimated_cov)\n",
    "    \n",
    "         # Assimilate all-at-once.\n",
    "        # -----------------------\n",
    "        mean_updated_aao_loc, ensemble_updated_aao_loc = my_filter.update_ensemble(\n",
    "            anomaly.data, ensemble_anomaly, G_mean,\n",
    "            d_mean, data_std, loc_estimated_cov)\n",
    "\n",
    "        # Trigger computations and block. Otherwise will clutter the scheduler. \n",
    "        mean_updated_aao_loc = client.persist(mean_updated_aao_loc)                \n",
    "        ensemble_updated_aao_loc = client.persist(ensemble_updated_aao_loc)\n",
    "        progress(ensemble_updated_aao_loc) # Block till end of computations.        \n",
    "    \n",
    "        # Save data.\n",
    "        np.save(os.path.join(results_folder, \"mean_updated_aao_loc_{}.npy\".format(assimilation_date)),\n",
    "            mean_updated_aao_loc.compute())\n",
    "        np.save(os.path.join(results_folder, \"ensemble_updated_aao_loc_{}.npy\".format(assimilation_date)),\n",
    "            ensemble_updated_aao_loc.compute())\n",
    "        \n",
    "        # Assimilate sequential.\n",
    "        # ----------------------\n",
    "        mean_updated_seq_loc, ensemble_updated_seq_loc = my_filter.update_ensemble_sequential_nondask(\n",
    "                anomaly.data, ensemble_anomaly, G_mean,\n",
    "                d_mean, data_std, localization_matrix)\n",
    "    \n",
    "        # Save data.\n",
    "        np.save(os.path.join(results_folder, \"mean_updated_seq_loc_{}.npy\".format(assimilation_date)),\n",
    "            mean_updated_seq_loc)\n",
    "        np.save(os.path.join(results_folder, \"ensemble_updated_seq_loc_{}.npy\".format(assimilation_date)),\n",
    "                ensemble_updated_seq_loc)\n",
    "        \n",
    "        # Compute scores. \n",
    "        # Before computing, have to put into unstacked form.\n",
    "        unstacked_updated_mean_aao_loc = helper_filter.dataset_mean.unstack_window_vector(mean_updated_aao_loc.compute(), time=assimilation_date, variable_name='temperature')\n",
    "        unstacked_updated_mean_seq_loc = helper_filter.dataset_mean.unstack_window_vector(mean_updated_seq_loc, time=assimilation_date, variable_name='temperature')\n",
    "        unstacked_updated_ensemble_aao_loc = helper_filter.dataset_members.unstack_window_vector(ensemble_updated_aao_loc.compute(), time=assimilation_date, variable_name='temperature')\n",
    "        unstacked_updated_ensemble_seq_loc = helper_filter.dataset_members.unstack_window_vector(ensemble_updated_seq_loc, time=assimilation_date, variable_name='temperature')\n",
    "        unstacked_prior = helper_filter.dataset_mean.unstack_window_vector(anomaly.values, time=assimilation_date, variable_name='temperature')\n",
    "        unstacked_prior_ens = helper_filter.dataset_members.unstack_window_vector(ensemble_anomaly.compute(), time=assimilation_date, variable_name='temperature')\n",
    "\n",
    "        # Load HadCRUT reference\n",
    "        ref_ds = xr.open_dataset(os.path.join(base_folder, \"Reference/HadCRUT.5.0.1.0.analysis.anomalies.ensemble_mean.nc\"))\n",
    "        if month == '02':\n",
    "            ref_date = '{}-{}-15'.format(year, month)\n",
    "        else: ref_date = assimilation_date\n",
    "        ref = ref_ds['tas_mean'].sel(time=ref_date)\n",
    "\n",
    "        # Regrid to common extent.\n",
    "        # Note that it was found out (see cornell_Nov_8_diagnose_stations.py) that regridding to a coarser grid (that of the reference), \n",
    "        # for comparison, lead to poor performances. The postulated reason for the discrepancy is that a coarse grid cell would contain \n",
    "        # too many highly different datapoints during assimilation.\n",
    "        #\n",
    "        # Hence, we instead regrid the reference to the finer (assimilation) grid.\n",
    "        regridded_ref = ref.isel(time=0).interp(\n",
    "            latitude=unstacked_updated_mean_aao_loc.latitude).interp(\n",
    "            longitude=unstacked_updated_mean_aao_loc.longitude)\n",
    "        stacked_ref = regridded_ref.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "\n",
    "        \"\"\"\n",
    "        regridded_prior = unstacked_prior.interp(latitude=ref.latitude).interp(longitude=ref.longitude)\n",
    "        regridded_prior_ens = unstacked_prior_ens.interp(latitude=ref.latitude).interp(longitude=ref.longitude)\n",
    "        regridded_mean_updated_aao_loc = unstacked_updated_mean_aao_loc.interp(latitude=ref.latitude).interp(longitude=ref.longitude)\n",
    "        regridded_mean_updated_seq_loc = unstacked_updated_mean_seq_loc.interp(latitude=ref.latitude).interp(longitude=ref.longitude)\n",
    "        regridded_ensemble_updated_aao_loc = unstacked_updated_ensemble_aao_loc.interp(latitude=ref.latitude).interp(longitude=ref.longitude)\n",
    "        regridded_ensemble_updated_seq_loc = unstacked_updated_ensemble_seq_loc.interp(latitude=ref.latitude).interp(longitude=ref.longitude)\n",
    "\n",
    "        # Now restack.\n",
    "        stacked_ref = ref.stack(stacked_dim=('latitude', 'longitude')).isel(time=0).compute()\n",
    "        stacked_prior = regridded_prior.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "        stacked_prior_ens = regridded_prior_ens.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "        stacked_mean_updated_aao_loc = regridded_mean_updated_aao_loc.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "        stacked_mean_updated_seq_loc = regridded_mean_updated_seq_loc.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "        stacked_ensemble_updated_aao_loc = regridded_ensemble_updated_aao_loc.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "        stacked_ensemble_updated_seq_loc = regridded_ensemble_updated_seq_loc.stack(stacked_dim=('latitude', 'longitude')).compute()\n",
    "        \"\"\"\n",
    "        stacked_prior = anomaly.values\n",
    "        stacked_prior_ens = ensemble_anomaly.compute()\n",
    "        stacked_mean_updated_aao_loc = mean_updated_aao_loc.compute()\n",
    "        stacked_mean_updated_seq_loc = mean_updated_seq_loc\n",
    "        stacked_ensemble_updated_aao_loc = ensemble_updated_aao_loc.compute()\n",
    "        stacked_ensemble_updated_seq_loc = ensemble_updated_seq_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6894ae-355a-4f40-b1c7-0ebf56855b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ES, _, _ = compute_energy_score(stacked_prior_ens, stacked_ref, min_lat=-70, max_lat=70)\n",
    "        ES_prior.append(ES)                                                     \n",
    "                                                                                \n",
    "        ES, _, _ = compute_energy_score(stacked_ensemble_updated_aao_loc, stacked_ref, min_lat=-70, max_lat=70)\n",
    "        ES_aao_loc.append(ES)                                                   \n",
    "                                                                                \n",
    "        ES, _, _ = compute_energy_score(stacked_ensemble_updated_seq_loc, stacked_ref, min_lat=-70, max_lat=70)\n",
    "        ES_seq_loc.append(ES)                                                   \n",
    "           \n",
    "        RE_score_map = compute_RE_score(stacked_prior, stacked_mean_updated_aao_loc, stacked_ref, min_lat=-70, max_lat=70)\n",
    "        RE = np.median(RE_score_map)\n",
    "        RE_aao_loc.append(RE)                                                   \n",
    "                                                                                \n",
    "        RE = np.median(compute_RE_score(stacked_prior, stacked_mean_updated_seq_loc, stacked_ref, min_lat=-70, max_lat=70))\n",
    "        RE_seq_loc.append(RE)                                                                                       \n",
    "\n",
    "        RMSE_prior.append(compute_RMSE(stacked_prior, stacked_ref, min_lat=-70, max_lat=70))\n",
    "        RMSE_aao_loc.append(compute_RMSE(stacked_mean_updated_aao_loc, stacked_ref, min_lat=-70, max_lat=70))\n",
    "        RMSE_seq_loc.append(compute_RMSE(stacked_mean_updated_seq_loc, stacked_ref, min_lat=-70, max_lat=70))\n",
    "        \n",
    "        dates.append(assimilation_date), months.append(month), years.append(year)\n",
    "                                                                                \n",
    "        df_results = pd.DataFrame({  \n",
    "                'date': dates, 'year': years, 'month': months,\n",
    "                'RMSE prior': RMSE_prior, 'RMSE aao loc': RMSE_aao_loc, 'RMSE seq loc': RMSE_seq_loc,\n",
    "                'ES prior': ES_prior, 'ES aao loc': ES_aao_loc, 'ES seq loc': ES_seq_loc,\n",
    "                'RE aao loc': RE_aao_loc, 'RE seq loc': RE_seq_loc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf2922-d020-417b-afd7-cec1195d3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "G = torch.from_numpy(G_mean.compute())\n",
    "_, obs_ind = (G[10, :]).reshape(1, -1).nonzero(as_tuple=True)\n",
    "print(_)\n",
    "print(obs_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec1517-df81-47ad-9203-5bd1afceb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb9f18-b29e-40ae-8d94-64bcfd426a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - df_results.iloc[3]['RMSE aao loc'] / df_results.iloc[3]['RMSE prior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ffc39-79c5-4a19-aec0-29057783e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(221)\n",
    "regridded_ref.plot(vmin=-6, vmax=6)\n",
    "# plt.scatter(data_ds.longitude, data_ds.latitude, c=data_ds.anomaly, cmap='viridis', s=10, alpha=0.5)\n",
    "plt.scatter(d_lons, d_lats, c=d_mean, cmap='viridis', s=10, alpha=0.5, vmin=-6, vmax=6)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"Reference\")\n",
    "\n",
    "plt.subplot(222)\n",
    "unstacked_prior.plot(vmin=-6, vmax=6)\n",
    "plt.scatter(d_lons, d_lats, c=d_mean, cmap='viridis', s=10, alpha=0.5, vmin=-6, vmax=6)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"Prior\")\n",
    "\n",
    "\n",
    "plt.subplot(223)\n",
    "unstacked_updated_mean_aao_loc.plot(vmin=-6, vmax=6)\n",
    "plt.scatter(d_lons, d_lats, c=d_mean, cmap='viridis', s=10, alpha=0.5, vmin=-6, vmax=6)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"Updated mean (aao)\")\n",
    "\n",
    "plt.subplot(224)\n",
    "unstacked_updated_mean_seq_loc.plot(vmin=-6, vmax=6)\n",
    "plt.scatter(d_lons, d_lats, c=d_mean, cmap='viridis', s=10, alpha=0.5, vmin=-6, vmax=6)\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"Updated mean (seq)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac33d5-6563-4a4b-b8d3-171ae2de8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592517d-1aba-479e-9b48-5eb6c66ce335",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ds['tas_mean'].sel(time=assimilation_date).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60680f8-bf69-4279-b960-30ef604f4b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9e5d1-6f87-4065-b0cc-c1559ceeb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "(stacked_ref.latitude > -75).data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
